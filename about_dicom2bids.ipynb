{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About dicom2bids\n",
    "dicom2bids is a python module for converting dicom files to nifti in a bids-compatible file structure. It uses dcm2niix for conversion, which must be installed separately (https://github.com/rordenlab/dcm2niix). It was developed at the University of Oregon and has not been tested on other systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicom folder structure\n",
    "dicom2bids assumes that dicoms are organized into separate folders by series, with folder names that include the series description. It provides a function for sorting your dicoms if they are not already organized in this manner. Arguments are input directory (pathlike object), output directory (pathlike object), and an optional flag to use or not use slurm (default False, requires slurmpy, may not work everywhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or more files already existing and not moved\n"
     ]
    }
   ],
   "source": [
    "# dicom sorting example\n",
    "import pathlib\n",
    "import dicom2bids\n",
    "unsorted_dicoms = pathlib.Path('/projects/lcni/jolinda/shared/TalapasClass/unsorted_dicoms/')\n",
    "dicom2bids.SortDicoms(unsorted_dicoms, 'sorted_dicoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_dicoms\n",
      "`-- phantom_20171212_161421\n",
      "    |-- Series_1_AAHScout\n",
      "    |-- Series_2_AAHScout_MPR_sag\n",
      "    |-- Series_3_AAHScout_MPR_cor\n",
      "    |-- Series_4_AAHScout_MPR_tra\n",
      "    `-- Series_5_Resting1\n",
      "\n",
      "6 directories\n"
     ]
    }
   ],
   "source": [
    "!tree sorted_dicoms -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping series descriptions to BIDS entities\n",
    "Any dicoms that are to be converted must have their series descriptions mapped to the appropriate BIDS entities. GetSeriesNames will extract all series descriptions from your sorted input dicom folder to make it easier to define this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAHScout_32ch-head-coil',\n",
       " 'AAHScout_32ch-head-coil_MPR_cor',\n",
       " 'AAHScout_32ch-head-coil_MPR_sag',\n",
       " 'AAHScout_32ch-head-coil_MPR_tra',\n",
       " 'Flair_axial.sw',\n",
       " 'bold_mb3_g2_2mm_te25',\n",
       " 'mprage_p2',\n",
       " 'pcasl_hires_0.0',\n",
       " 'pcasl_hires_0.2',\n",
       " 'pcasl_hires_0.7',\n",
       " 'pcasl_hires_1.2',\n",
       " 'pcasl_hires_1.7',\n",
       " 'pcasl_hires_2.2',\n",
       " 'se_epi_mb3_g2_2mm_ap',\n",
       " 'se_epi_mb3_g2_2mm_pa',\n",
       " 'siemens_diff_3shell_ap',\n",
       " 'siemens_diff_3shell_lr',\n",
       " 'siemens_diff_3shell_pa',\n",
       " 'siemens_diff_3shell_rl',\n",
       " 't2_space_sag_p2_iso',\n",
       " 't2_tse_cor65slice_2avg.+',\n",
       " 'tof_fl3d_tra_p2_multi-slab',\n",
       " 'tof_fl3d_tra_p2_multi-slab_MIP_COR',\n",
       " 'tof_fl3d_tra_p2_multi-slab_MIP_SAG',\n",
       " 'tof_fl3d_tra_p2_multi-slab_MIP_TRA'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dicoms = pathlib.Path('/projects/lcni/dcm/lcni/Burggren/HEAT')\n",
    "dicom2bids.GetSeriesNames(example_dicoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should be mapped to output file names using a dictionary. 'datatype' and 'suffix' are required. dicom2bids does NOT check whether all required fields have been defined (eg, 'task' for bold images), that's up to you. Entries can be defined in any order; dicom2bids will format the output file names correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = dicom2bids.bids_dict() # create a bids dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.add('mprage_p2', datatype = 'anat', suffix = 'T1w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.add('bold_mb3_g2_2mm_te25', datatype = 'func', suffix = 'bold', task = 'resting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some BIDS entities are also python keywords. In that case you can't use this function call:\n",
    "``bd.add('siemens_diff_3shell_ap', datatype = 'fmap', suffix = 'epi', dir = 'ap') ``\n",
    "Instead, use the \"entities\" parameter. This parameter takes a dictionary as an argument:\n",
    "``bd.add('siemens_diff_3shell_ap', datatype = 'fmap', suffix = 'epi', entities = {'dir':'ap'}) ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.add('siemens_diff_3shell_ap', datatype = 'fmap', suffix = 'epi', entities = {'dir':'ap'})\n",
    "bd.add('siemens_diff_3shell_pa', datatype = 'fmap', suffix = 'epi', entities = {'dir':'pa'})\n",
    "bd.add('siemens_diff_3shell_rl', datatype = 'fmap', suffix = 'epi', entities = {'dir':'rl'})\n",
    "bd.add('siemens_diff_3shell_lr', datatype = 'fmap', suffix = 'epi', entities = {'dir':'lr'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert things that aren't in the bids standard by including the \"nonstandard = True\" argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.add('pcasl_hires_0.0', datatype = 'perf', suffix = 'asl', acq = '0.0', nonstandard = True)\n",
    "bd.add('pcasl_hires_0.2', datatype = 'perf', suffix = 'asl', acq = '0.2', nonstandard = True)\n",
    "bd.add('pcasl_hires_0.7', datatype = 'perf', suffix = 'asl', acq = '0.7', nonstandard = True)\n",
    "bd.add('pcasl_hires_1.2', datatype = 'perf', suffix = 'asl', acq = '1.2', nonstandard = True)\n",
    "bd.add('pcasl_hires_1.7', datatype = 'perf', suffix = 'asl', acq = '1.7', nonstandard = True)\n",
    "bd.add('pcasl_hires_2.2', datatype = 'perf', suffix = 'asl', acq = '2.2', nonstandard = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprage_p2: sub-{}_run-{}_T1w\n",
      "bold_mb3_g2_2mm_te25: sub-{}_task-resting_run-{}_bold\n",
      "siemens_diff_3shell_ap: sub-{}_dir-ap_run-{}_epi\n",
      "siemens_diff_3shell_pa: sub-{}_dir-pa_run-{}_epi\n",
      "siemens_diff_3shell_rl: sub-{}_dir-rl_run-{}_epi\n",
      "siemens_diff_3shell_lr: sub-{}_dir-lr_run-{}_epi\n",
      "pcasl_hires_0.0: sub-{}_acq-0.0_run-{}_asl\n",
      "pcasl_hires_0.2: sub-{}_acq-0.2_run-{}_asl\n",
      "pcasl_hires_0.7: sub-{}_acq-0.7_run-{}_asl\n",
      "pcasl_hires_1.2: sub-{}_acq-1.2_run-{}_asl\n",
      "pcasl_hires_1.7: sub-{}_acq-1.7_run-{}_asl\n",
      "pcasl_hires_2.2: sub-{}_acq-2.2_run-{}_asl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the 'run' entity\n",
    "'run' will be replaced with the series number from the dicom file. This ensures that every run, including duplicates, will be converted, and that the output files are still BIDS compliant. If you want to use 'run' differently, or not use it at all, you'll need to rename your files after conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion\n",
    "Once you've defined your bids dictionary, call Convert with the input directory, output directory, and bids dictionary. Optionally, you can add \"slurm = True\" to submit conversion as a job to the cluster (this requires slurmpy and may not work on all or even most clusters, but if it works it will be MUCH faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output directory\n",
    "bidsdir = pathlib.Path.home() / 'lcni' / 'bidsexample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about duplicate subjects\n",
    "The current iteration of dicom2bids allows you to submit a directory with multiple subjects, but it assumes that there's only one of each! In my example input I have two sessions for subject 999 and that's a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/lcni/dcm/lcni/Burggren/HEAT\n",
      "|-- HEAT002_20200303_102436\n",
      "|-- HEAT_999_20191211_105824\n",
      "|-- HEAT_999_20200127_132053\n",
      "`-- Phantom_20200116_151959\n",
      "\n",
      "4 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree {str(example_dicoms)} -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a bit of python to only select the 2020 subjects, and convert one directory at a time instead of running `dicom2bids.Convert(example_dicoms, bidsdir, bd)`. If this was a real study, I'd probably add the \"ses\" keyword to my dictionary, split my list of subject directories to convert into two lists, and have one with ses = 'one' in the bids dictionary and one with ses = 'two'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/lcni/dcm/lcni/Burggren/HEAT/HEAT_999_20200127_132053\n",
      "/projects/lcni/dcm/lcni/Burggren/HEAT/Phantom_20200116_151959\n",
      "/projects/lcni/dcm/lcni/Burggren/HEAT/HEAT002_20200303_102436\n"
     ]
    }
   ],
   "source": [
    "for subjectdir in example_dicoms.glob('*_2020*'):\n",
    "    print(subjectdir)\n",
    "    dicom2bids.Convert(subjectdir, bidsdir, bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bidsexample\n",
      "|-- dataset_description.json\n",
      "|-- participants.json\n",
      "|-- participants.tsv\n",
      "|-- sub-HEAT002\n",
      "|   |-- anat\n",
      "|   |   |-- sub-HEAT002_run-07_T1w.json\n",
      "|   |   `-- sub-HEAT002_run-07_T1w.nii.gz\n",
      "|   |-- fmap\n",
      "|   |   |-- sub-HEAT002_dir-ap_run-17_epi.bval\n",
      "|   |   |-- sub-HEAT002_dir-ap_run-17_epi.bvec\n",
      "|   |   |-- sub-HEAT002_dir-ap_run-17_epi.json\n",
      "|   |   |-- sub-HEAT002_dir-ap_run-17_epi.nii.gz\n",
      "|   |   |-- sub-HEAT002_dir-lr_run-20_epi.bval\n",
      "|   |   |-- sub-HEAT002_dir-lr_run-20_epi.bvec\n",
      "|   |   |-- sub-HEAT002_dir-lr_run-20_epi.json\n",
      "|   |   |-- sub-HEAT002_dir-lr_run-20_epi.nii.gz\n",
      "|   |   |-- sub-HEAT002_dir-pa_run-18_epi.bval\n",
      "|   |   |-- sub-HEAT002_dir-pa_run-18_epi.bvec\n",
      "|   |   |-- sub-HEAT002_dir-pa_run-18_epi.json\n",
      "|   |   |-- sub-HEAT002_dir-pa_run-18_epi.nii.gz\n",
      "|   |   |-- sub-HEAT002_dir-rl_run-19_epi.bval\n",
      "|   |   |-- sub-HEAT002_dir-rl_run-19_epi.bvec\n",
      "|   |   |-- sub-HEAT002_dir-rl_run-19_epi.json\n",
      "|   |   `-- sub-HEAT002_dir-rl_run-19_epi.nii.gz\n",
      "|   |-- func\n",
      "|   |   |-- sub-HEAT002_task-resting_run-10_bold.json\n",
      "|   |   `-- sub-HEAT002_task-resting_run-10_bold.nii.gz\n",
      "|   `-- perf\n",
      "|       |-- sub-HEAT002_acq-0.0_run-16_asl.json\n",
      "|       |-- sub-HEAT002_acq-0.0_run-16_asl.nii.gz\n",
      "|       |-- sub-HEAT002_acq-0.2_run-15_asl.json\n",
      "|       |-- sub-HEAT002_acq-0.2_run-15_asl.nii.gz\n",
      "|       |-- sub-HEAT002_acq-0.7_run-14_asl.json\n",
      "|       |-- sub-HEAT002_acq-0.7_run-14_asl.nii.gz\n",
      "|       |-- sub-HEAT002_acq-1.2_run-13_asl.json\n",
      "|       |-- sub-HEAT002_acq-1.2_run-13_asl.nii.gz\n",
      "|       |-- sub-HEAT002_acq-1.7_run-12_asl.json\n",
      "|       |-- sub-HEAT002_acq-1.7_run-12_asl.nii.gz\n",
      "|       |-- sub-HEAT002_acq-2.2_run-11_asl.json\n",
      "|       `-- sub-HEAT002_acq-2.2_run-11_asl.nii.gz\n",
      "|-- sub-HEAT999\n",
      "|   |-- anat\n",
      "|   |   |-- sub-HEAT999_run-07_T1w.json\n",
      "|   |   `-- sub-HEAT999_run-07_T1w.nii.gz\n",
      "|   |-- fmap\n",
      "|   |   |-- sub-HEAT999_dir-ap_run-11_epi.bval\n",
      "|   |   |-- sub-HEAT999_dir-ap_run-11_epi.bvec\n",
      "|   |   |-- sub-HEAT999_dir-ap_run-11_epi.json\n",
      "|   |   |-- sub-HEAT999_dir-ap_run-11_epi.nii.gz\n",
      "|   |   |-- sub-HEAT999_dir-lr_run-14_epi.bval\n",
      "|   |   |-- sub-HEAT999_dir-lr_run-14_epi.bvec\n",
      "|   |   |-- sub-HEAT999_dir-lr_run-14_epi.json\n",
      "|   |   |-- sub-HEAT999_dir-lr_run-14_epi.nii.gz\n",
      "|   |   |-- sub-HEAT999_dir-pa_run-12_epi.bval\n",
      "|   |   |-- sub-HEAT999_dir-pa_run-12_epi.bvec\n",
      "|   |   |-- sub-HEAT999_dir-pa_run-12_epi.json\n",
      "|   |   |-- sub-HEAT999_dir-pa_run-12_epi.nii.gz\n",
      "|   |   |-- sub-HEAT999_dir-rl_run-13_epi.bval\n",
      "|   |   |-- sub-HEAT999_dir-rl_run-13_epi.bvec\n",
      "|   |   |-- sub-HEAT999_dir-rl_run-13_epi.json\n",
      "|   |   `-- sub-HEAT999_dir-rl_run-13_epi.nii.gz\n",
      "|   |-- func\n",
      "|   |   |-- sub-HEAT999_task-resting_run-10_bold.json\n",
      "|   |   `-- sub-HEAT999_task-resting_run-10_bold.nii.gz\n",
      "|   `-- perf\n",
      "|       |-- sub-HEAT999_acq-0.0_run-21_asl.json\n",
      "|       |-- sub-HEAT999_acq-0.0_run-21_asl.nii.gz\n",
      "|       |-- sub-HEAT999_acq-0.2_run-20_asl.json\n",
      "|       |-- sub-HEAT999_acq-0.2_run-20_asl.nii.gz\n",
      "|       |-- sub-HEAT999_acq-0.7_run-19_asl.json\n",
      "|       |-- sub-HEAT999_acq-0.7_run-19_asl.nii.gz\n",
      "|       |-- sub-HEAT999_acq-1.2_run-18_asl.json\n",
      "|       |-- sub-HEAT999_acq-1.2_run-18_asl.nii.gz\n",
      "|       |-- sub-HEAT999_acq-1.7_run-17_asl.json\n",
      "|       |-- sub-HEAT999_acq-1.7_run-17_asl.nii.gz\n",
      "|       |-- sub-HEAT999_acq-2.2_run-16_asl.json\n",
      "|       `-- sub-HEAT999_acq-2.2_run-16_asl.nii.gz\n",
      "`-- sub-Phantom\n",
      "    `-- func\n",
      "        |-- sub-Phantom_task-resting_run-05_bold.json\n",
      "        `-- sub-Phantom_task-resting_run-05_bold.nii.gz\n",
      "\n",
      "12 directories, 69 files\n"
     ]
    }
   ],
   "source": [
    "!tree {str(bidsdir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_id\tage\tsex\n",
      "sub-HEAT999\t25\tM\n",
      "sub-Phantom\t24\tO\n",
      "sub-HEAT002\t47\tF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we automatically created a participants file\n",
    "with open(bidsdir / 'participants.tsv') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"age\": {\n",
      "        \"Description\": \"age of participant\",\n",
      "        \"Units\": \"years\"\n",
      "    },\n",
      "    \"sex\": {\n",
      "        \"Description\": \"sex of participant\",\n",
      "        \"Levels\": {\n",
      "            \"M\": \"male\",\n",
      "            \"F\": \"female\",\n",
      "            \"O\": \"other\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(bidsdir / 'participants.json') as f:\n",
    "    print(json.dumps(json.load(f), indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also creates a dataset_description file for you. On talapas it will even attempt to fill out the authors with your name and the PI on the project (using the pirg structure in the dicom repository; in this example it gets it wrong but we can always go back and edit it later). If you don't want either of these files created just specify description_file = False and/or participant_file = False in your call to dicom2bids.Convert()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Name\": \"HEAT\",\n",
      "    \"BIDSVersion\": \"1.3.0\",\n",
      "    \"Authors\": [\n",
      "        \"Fred Sabb\",\n",
      "        \"Jolinda Smith\"\n",
      "    ],\n",
      "    \"Acknowledgements\": \"BIDS conversion was performed using dcm2niix and dicom2bids.\",\n",
      "    \"ReferencesAndLinks\": [\n",
      "        \"Li X, Morgan PS, Ashburner J, Smith J, Rorden C (2016) The first step for neuroimaging data analysis: DICOM to NIfTI conversion. J Neurosci Methods. 264:47-56. doi: 10.1016/j.jneumeth.2016.03.001.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(bidsdir / 'dataset_description.json') as f:\n",
    "    print(json.dumps(json.load(f), indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing -- in our dicom files, certain fields used by dcm2niix to write the .json files are wrong. We can fix this if we know what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewis_Building\n",
      "Department\n",
      "Franklin_Blvd_1440_Eugene_District_US_97403\n"
     ]
    }
   ],
   "source": [
    "with open(next(bidsdir.rglob('sub*.json'))) as f:\n",
    "    j = json.load(f)\n",
    "    print(j['InstitutionName'])\n",
    "    print(j['InstitutionalDepartmentName'])\n",
    "    print(j['InstitutionAddress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a dictionary object with the correct values; this particular problem is pervasive at LCNI so I've included it in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InstitutionName': 'University of Oregon',\n",
       " 'InstitutionalDepartmentName': 'LCNI',\n",
       " 'InstitutionAddress': 'Franklin_Blvd_1440_Eugene_Oregon_US_97403'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom2bids.lcni_corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert again and include this in the argument \"json_mod\". Since we only need to change the .json files, we can include the dcm2niix flags '-b o -w 0' to skip converting the .nii.gz files. We don't have to set particpant_file = False and description_file = False, they'll come through unchanged, but I will for illustration purposes. (-w 0 is 'ignore duplicates'. You might think we want -w 1, 'overwrite', but that will delete the existing dicom files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/lcni/dcm/lcni/Burggren/HEAT/HEAT_999_20200127_132053\n",
      "/projects/lcni/dcm/lcni/Burggren/HEAT/Phantom_20200116_151959\n",
      "/projects/lcni/dcm/lcni/Burggren/HEAT/HEAT002_20200303_102436\n"
     ]
    }
   ],
   "source": [
    "for subjectdir in example_dicoms.glob('*_2020*'):\n",
    "    print(subjectdir)\n",
    "    dicom2bids.Convert(subjectdir, bidsdir, bd, json_mod = dicom2bids.lcni_corrections, dcm2niix_flags= '-b o -w 0', participant_file = False, description_file = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Oregon\n",
      "LCNI\n",
      "Franklin_Blvd_1440_Eugene_Oregon_US_97403\n"
     ]
    }
   ],
   "source": [
    "with open(next(bidsdir.rglob('sub*.json'))) as f:\n",
    "    j = json.load(f)\n",
    "    print(j['InstitutionName'])\n",
    "    print(j['InstitutionalDepartmentName'])\n",
    "    print(j['InstitutionAddress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
